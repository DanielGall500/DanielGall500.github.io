---
title: "The Diffusion-based Generation of Truth"
date: 2025-05-25
draft: false
garden_tags: ["Neural Networks", "Machine Learning", "AI"]
summary: "The implications of work on Google's new video generation model Veo 3 are potentially far-reaching."
weight: 1
---

>"Through photography, the world becomes a series of unrelated, free-standing particles; and history, past and present, a set of anecdotes and faits divers. The camera makes reality **atomic**, **manageable**, and **opaque**." Susan Sontag, On Photography (P. 23)

In the book *Views* by German author Marc Uwe-Kling, a video goes viral on the internet showing explicit content involving a missing person. The book follows a detective on this case attempting to get to find the truth contained within this video. Who are the people involved? Why are they doing this? Where are they located? A slow revelation over the course of the book is that the video is not real, it is entirely AI generated based off of images on the user's social media account. There is nothing about the video that can give any indication of what happened to the missing person involved, and the time looking into it has been wasted.

With the release of [Google DeepMind's Veo](https://deepmind.google/models/veo/), this reality appears to be approaching at a rapid pace. It includes a short video of a high-definition fisherman which can still be identified as unreal, but only because of the fact that the quality is so high. At a lower resolution these lines may be further blurred such as the typically low resolution of CCTV cameras. The model uses a mixture of *diffusion* and *transformer* architectures. The diffusion component begins by starting with very blurry frames and attempts to move them closer and closer to the desired prompt. The transformer component is the basis for many of the recent advances in language-generation due to its strong ability to take into account context, particular over long distances, where distance may be measured in *tokens* in a language model and *frames* in a video model.

Let's take for instance a seemingly harmless example: a particular child kicking a football. Given the goal of creating a video of this child performing this action, let's say we have the following inputs:

1. A suitable prompt to elicit the desired action. <br>
2. An image or collection of images of the child.<br> 

We can define the probability of the Veo model producing a believable video as α, where a believable video is one which, when presented to a person who knows this child, they will on average believe that it is real. A decade ago, α would have been negligible except for the lowest quality of videos. Even then, multi-modal inputs such as text and an image of a face were much less common and prompts did not wield the same accuracy as they do now. In recent years however, α appears to have been gradually increasing further and further. The closer that this number gets to 1, the closer we come to the generation of new realities for ourselves, new actions that we've performed and new things that we can be accused of. That is, the automatic generation of events which did not happen, but which we will accept for reality. This is more than just a lie, as it relies on something which we take for granted: that video footage and photography give us a glimpse into an event as it really happened.

> "The force of photography is that it keeps open to scrutiny instants which the normal flow of time immediately replaces." - Susan Sontag, On Photography (P.110) 

We are arguably moving passed a time where we were most able to observe and scrutinise the truth. Despite the long-time ability of photographs to be edited to adjust reality (see [Joseph Stalin's image adjustments](https://en.wikipedia.org/wiki/Censorship_of_images_in_the_Soviet_Union)), the ability to generate *new* photographs of *new* realities has not been a threat until recently. Adjustments were always an issue, but some basis of truth remained in the photo to be seen. Videos provided an even stronger basis for truth, they were difficult to adjust and impossible to generate. While truth could be further obscured by these media, they provided our strongest source of hope to find it in many cases. Editing such videos to appear real was a costly endeavour in the past but it is now becoming democratised with minimal barriers to entry. As α becomes higher, the erosion of trust in these media will become more apparent. The strength of evidence in the courts becomes weakened, the ability of bad actors to manipulate audiences with apparent but fake truths becomes stronger, and the harms of automatic content generation will target those who can do nothing to stop it such as the generation of explicit content as in Views.

Auto-generated video identification tools such as [DeepMind's AI watermark tool SynthID](https://deepmind.google/science/synthid/) provide some hope for the possibility of discerning real from fake. However, if these models are going to continuously improve as they have been, these sorts of safety mechanisms need to become much more prioritised by companies than they currently are. Furthermore, a tool for the verification of AI generated content which is a black box and privately owned comes with its own additional risks. With the current rate of performance improvements in this field, it is imperative that companies and regulatory bodies think pessimistically about the potential use cases of these models; a tool for the generation of new realities should not be presented as a fluffy purple creature.